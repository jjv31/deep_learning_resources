{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94c25c5-320d-443e-80bc-95da0afa7b2a",
   "metadata": {},
   "source": [
    "# Final Project, Part 2: Forecasting Weather via LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c864f-5f73-4619-86e2-cff92fb113fd",
   "metadata": {},
   "source": [
    "Time series datasets are defined as databases that contain a sequence of datapoints over time. This includes stock prices (e.g., price per day), weather (e.g., degrees Celsius per day), and sales figures (net profit by quarter), among others.\n",
    "\n",
    "We'll be using LSTMs to predict the weather of Melbourne, Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dafafe-31be-41e1-ba6d-4050f78fad67",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/final_project/weather_forecasting_icon.png\" width=\"25%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c277da-620b-430a-99dc-5757acf9181b",
   "metadata": {},
   "source": [
    "However, our current LSTM neural network performs terribly! Thus, you'll be improving it and present your findings to the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f482ea3-7c15-4e64-a844-5d82688b97dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 0 | Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b248c-3e72-483f-bd98-e47f0fee2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b10fc6-ad24-4a6f-bcf5-17a7a2ae6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_safe(src, dst, max_len=200):\n",
    "    \"\"\"Copy files, skip long paths\"\"\"\n",
    "    skipped = 0\n",
    "    for root, dirs, files in os.walk(src):\n",
    "        rel_path = os.path.relpath(root, src)\n",
    "        dst_root = os.path.join(dst, rel_path) if rel_path != '.' else dst\n",
    "        if len(dst_root) < max_len:\n",
    "            os.makedirs(dst_root, exist_ok=True)\n",
    "            for file in files:\n",
    "                dst_file = os.path.join(dst_root, file)\n",
    "                if len(dst_file) < max_len:\n",
    "                    try: shutil.copy2(os.path.join(root, file), dst_file)\n",
    "                    except: skipped += 1\n",
    "                else: skipped += 1\n",
    "        else: skipped += len(files)\n",
    "    return skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0050df-d63e-48df-8424-130a190d0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup resources if needed\n",
    "setup_ran = False\n",
    "if not os.path.exists('res'):\n",
    "    print(\"Setting up resources...\")\n",
    "    setup_ran = True\n",
    "    \n",
    "    # Cleanup, clone, copy\n",
    "    repo = 'deep_learning_resources'\n",
    "    if os.path.exists(repo):\n",
    "        shutil.rmtree(repo, onerror=lambda f,p,e: os.chmod(p, stat.S_IWRITE) or f(p))\n",
    "    \n",
    "    !git clone --depth=1 https://github.com/jjv31/deep_learning_resources\n",
    "    \n",
    "    if os.path.exists(f'{repo}/res'):\n",
    "        skipped = copy_safe(f'{repo}/res', 'res')\n",
    "        print(f\"Setup complete! {'(' + str(skipped) + ' long filenames skipped)' if skipped else ''}\")\n",
    "    \n",
    "    shutil.rmtree(repo, onerror=lambda f,p,e: os.chmod(p, stat.S_IWRITE) or f(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3540a59-a5c3-48cd-aeba-bbc6ca58c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only refresh if we just downloaded resources\n",
    "if setup_ran:\n",
    "    from IPython.display import Javascript, display\n",
    "    import time\n",
    "    \n",
    "    print(\"Refreshing images...\")\n",
    "    \n",
    "    # Try browser refresh + aggressive image reload\n",
    "    display(Javascript(f'''\n",
    "    try {{ setTimeout(() => window.location.reload(true), 2000); }} catch(e) {{}}\n",
    "    \n",
    "    const t = {int(time.time())};\n",
    "    document.querySelectorAll('img').forEach((img, i) => {{\n",
    "        if (img.src.includes('res/')) {{\n",
    "            const src = img.src.split('?')[0];\n",
    "            setTimeout(() => img.src = src + '?v=' + t + '_' + i, i * 50);\n",
    "        }}\n",
    "    }});\n",
    "    '''))\n",
    "    \n",
    "    print(\"If images don't appear, press Ctrl+Shift+R to hard refresh!\")\n",
    "else:\n",
    "    print(\"Resources already exist, skipping setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ff39f-2fa1-4f1f-b925-ac2c9aae92ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1 | Loads & Inspects Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259ad2c-05ef-4b78-b810-a5bbf377a9a5",
   "metadata": {},
   "source": [
    "### 1.1 | Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdcdfe6-35b4-4bac-98fe-c56256eb91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Other\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Neural Nets\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a06c6-70aa-4d66-9d21-c0c699bc2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f622802-2304-45c6-add9-e4154367e06a",
   "metadata": {},
   "source": [
    "### 1.2 | Aux functions. Just run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b78be0-4015-4f39-bd3e-16b3ba28f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to facilitate evaluating our models\n",
    "def print_score(clf, X, y_true):\n",
    "\n",
    "    # Gets predicted labels\n",
    "    if isinstance(clf, keras.models.Sequential): # If the model is a Keras neural network\n",
    "        y_pred = (clf.predict(X) >= 0.5).astype(int) \n",
    "    else: # Normal scikit-learn model\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "    # Gets key performance indicators\n",
    "    accuracy = round(accuracy_score(y_true, y_pred), 4)\n",
    "    recall = round(recall_score(y_true, y_pred), 4)\n",
    "    precision = round(precision_score(y_true, y_pred), 4)\n",
    "    f1 = round(f1_score(y_true, y_pred), 4)\n",
    "\n",
    "    # Displays them\n",
    "    print(f\"F1 = {f1:.4f} | Recall = {recall* 100:.2f}% | Precision = {precision*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c274b7c-8d2e-4bf3-b3de-939ee1df2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the performance of the neural network\n",
    "def plot_performance(training_values, validation_values, metric_name = \"Recall\"):\n",
    "\n",
    "    epochs = range(1, len(training_values) + 1)\n",
    "    \n",
    "    sns.set() \n",
    "    plt.plot(epochs, training_values, '-', label=f'Training {metric_name}')\n",
    "    plt.plot(epochs, validation_values, ':', label=f'Validation {metric_name}')\n",
    "\n",
    "    plt.title(f'Training and Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa1d790-5103-406c-9cbc-3839c90ace16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a time series dataframe and returns a list of missing dates\n",
    "# Assumes one day increments\n",
    "def get_missing_dates(df):\n",
    "    \n",
    "    # Check for missing dates\n",
    "    full_date_range = pd.date_range(start=df['Date'].min(), end=df['Date'].max(), freq='D')\n",
    "\n",
    "    # Convert the existing dates in your DataFrame to a set for efficient lookup\n",
    "    existing_dates = set(df['Date'])\n",
    "\n",
    "    # Find the dates that are in the full_date_range but not in your existing dates\n",
    "    missing_dates = [date for date in full_date_range if date not in existing_dates]\n",
    "\n",
    "    return missing_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33298dc2-73ad-4d01-9b26-c6f0cf409f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_univariates_metric(data, nameToPrint=None):\n",
    "\n",
    "    # Mode - Handling multimodal cases\n",
    "    mode_result = data.mode()\n",
    "    if len(mode_result) == 0:  # No mode found\n",
    "        mode_result = None\n",
    "    else:\n",
    "        mode_result = mode_result[0]\n",
    "\n",
    "    # Print output\n",
    "    print(f\"Descriptives for {nameToPrint}\")\n",
    "    print(f\"Mean = {round(data.mean(),2)} | Median = {round(data.median(),2)} | Mode = {mode_result} | \"\n",
    "          f\"Min = {data.min()} | Max = {data.max()} | SD = {round(data.std(),2)} | \"\n",
    "          f\"IQR(25) = {data.quantile(0.25)} | IQR(75) = {data.quantile(0.75)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724076a3-8bb9-4325-a98f-1f9e7e580273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the results of a time series.\n",
    "# model is the neural network\n",
    "# generator_to_evaluate is either the generator trained on your training or testing set\n",
    "# scaler is the min-max scaler trained in §2.\n",
    "def plot_time_series_results(model, generator_to_evaluate, scaler, date_index):\n",
    "\n",
    "    # Returns pandas dataframe that contains (i) actual value (milk production) and (ii) predicted value. Both descaled.\n",
    "    def get_results_df():\n",
    "        \n",
    "        # Creates list to store  (i) predicted and (ii) actual values\n",
    "        all_predictions = []\n",
    "        all_actuals = []\n",
    "\n",
    "        for i in range(len(generator_to_evaluate)):\n",
    "            # Gets a batch of (i) X features and (i) the target they're trying to predict\n",
    "            x_batch, y_batch = generator_to_evaluate[i]\n",
    "\n",
    "            # Make predictions on the current batch\n",
    "            batch_predictions = model.predict(x_batch, verbose=0)\n",
    "\n",
    "            # Extend our lists with the current batch's predictions and actuals\n",
    "            # Flatten them if they are in shape (batch_size, 1) to (batch_size,)\n",
    "            all_predictions.extend(batch_predictions.flatten())\n",
    "            all_actuals.extend(y_batch.flatten())\n",
    "\n",
    "        # Convert lists to NumPy arrays for easier manipulation\n",
    "        all_predictions = np.array(all_predictions).reshape(-1, 1) # Reshape back to (n_samples, 1) for inverse_transform\n",
    "        all_actuals = np.array(all_actuals).reshape(-1, 1)\n",
    "\n",
    "        # Descales predictions (via the scaler) so they're intelligible again (i.e., not approximately 0-1)\n",
    "        all_predictions = scaler.inverse_transform(all_predictions)\n",
    "        all_actuals = scaler.inverse_transform(all_actuals)\n",
    "\n",
    "\n",
    "        # Create a DataFrame for easy viewing\n",
    "        results_df = pd.DataFrame({'Actual': all_actuals.flatten(), 'Predicted': all_predictions.flatten()}, \n",
    "                                  index=date_index[n_input:])\n",
    "        return results_df\n",
    "\n",
    "    # Plots the results df. Takes the results_df returned in the above subfunction.\n",
    "    def plot_results_df(results_df):\n",
    "\n",
    "        # Defines plot size\n",
    "        plt.figure(figsize=(7, 5))\n",
    "\n",
    "        # Plots vals\n",
    "        plt.plot(results_df['Actual'], label='Actual Values (Test Set)', color='blue', linewidth=0.5, marker='o', markersize=1)\n",
    "        plt.plot(results_df['Predicted'], label='Predicted Values (Test Set)', color='red', linestyle='--', linewidth=0.5, marker='o', markersize=1)\n",
    "\n",
    "        \n",
    "        # Labels\n",
    "        plt.title('LSTM Model Predictions vs. Actuals')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Temperature (Celsius)')\n",
    "\n",
    "        # Other\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # Main functions\n",
    "    results_df = get_results_df()\n",
    "    plot_results_df(results_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d62b9-73f2-49eb-8f45-e706b52a723e",
   "metadata": {},
   "source": [
    "### 1.3 | Loads & Inspects Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fd8a0-d8c0-4bb1-8b9e-dd2aee6216aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now load the data using the pandas dataframe. We will use milk production data\n",
    "df = pd.read_csv('res/final_project/temperature.csv')\n",
    "\n",
    "# Convert 'Date' column to datetime objects with the correct format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y')\n",
    "\n",
    "# Convert 'Temp' column to numeric, coercing errors to NaN\n",
    "df['Temp'] = pd.to_numeric(df['Temp'], errors='coerce')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e9ffa-64c8-4cdb-9348-d9274b2e044f",
   "metadata": {},
   "source": [
    "### 1.4 | Imputes Missing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e836ff2-3038-4ade-ad22-6435a3f4242a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stores imputed values\n",
    "imputed_dates_dict = {}\n",
    "\n",
    "# Gets (i) dates missing from Pandas and (ii) dates with a null/no temperature\n",
    "missing_dates = get_missing_dates(df)\n",
    "null_dates = df[df['Temp'].isnull()][\"Date\"].tolist() \n",
    "\n",
    "for ind_date in (missing_dates + null_dates) :\n",
    "\n",
    "    # Gets temp before & after the missing date\n",
    "    date_before, date_after = ind_date - pd.Timedelta(days=1), ind_date + pd.Timedelta(days=1)\n",
    "    temp_before, temp_after = df[df[\"Date\"] == date_before][\"Temp\"].iloc[0], df[df[\"Date\"] == date_after][\"Temp\"].iloc[0]\n",
    "\n",
    "    # Calculates average temp\n",
    "    if np.isnan(temp_before) and np.isnan(temp_after):\n",
    "        average_temp = np.nan\n",
    "    elif np.isnan(temp_before):\n",
    "        average_temp = round(temp_after, 1)\n",
    "    elif np.isnan(temp_after):\n",
    "        average_temp = round(temp_before, 1)\n",
    "    else:\n",
    "        average_temp = round((temp_before + temp_after) / 2, 1)\n",
    "\n",
    "    # Displays the imputation process to the user\n",
    "    print(f\"The following date [{ind_date}] has a missing temperature.\")\n",
    "    print(f\"--> The temperature the day before [{date_before}] was [{temp_before}]\")\n",
    "    print(f\"--> The temperature the day after [{date_after}] was [{temp_after}]\")\n",
    "    print(f\"--> Thus, we'll infer the temperature of that date is the average of the two: {average_temp}\\n\")\n",
    "\n",
    "    # Stores imputation\n",
    "    imputed_dates_dict[ind_date] = average_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8bfa65-8319-42c8-86fa-ba71b02ceb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops the null dates so they can be re-added\n",
    "df = df[~df['Date'].isin(null_dates)]\n",
    "\n",
    "# Stores imputed dates into our pandas df\n",
    "imputed_df = pd.DataFrame(imputed_dates_dict.items(), columns=['Date', 'Temp'])\n",
    "df = pd.concat([df, imputed_df], ignore_index=True)\n",
    "df = df.sort_values(by='Date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e10386-7e5e-49a5-9389-b15ec48ea30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures all misisng dates are imputed\n",
    "assert( len(get_missing_dates(df)) == 0 )\n",
    "print(\"Congratulations! There are no more missing dates!\")\n",
    "\n",
    "assert( len( df[df['Temp'].isnull()][\"Date\"].tolist() )  == 0 )\n",
    "print(\"Congratulations! There are no more null dates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aee05e-5197-4c40-96ea-7bf5eec3305c",
   "metadata": {},
   "source": [
    "### 1.5 | Visualizes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a61532-a588-4316-b63e-50df776b43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5f3ae-953b-4cb0-abe0-73f2bc2451f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting graph b/w production and date\n",
    "df.plot(figsize=(20, 6), linewidth=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d843f-7b9e-4a18-84cc-37f598a4b1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Same as above, except it plots seasonality, trends, and noise.\n",
    "# Noise is defined as time series datapoint - trend - seasonality\n",
    "seasonal_decompose(df['Temp']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ab85f-9f87-482c-a0dc-6e79f4e281de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2 | Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f91d1-f98f-465d-a3c9-5f509abba262",
   "metadata": {},
   "source": [
    "### 2.1 | Scales Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece1eb7-da4d-44fd-8902-a86ed4e21b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "print_univariates_metric(df[\"Temp\"], \"Temperature before scaling\")\n",
    "df[\"Temp\"] = scaler.fit_transform(df[[\"Temp\"]])\n",
    "print_univariates_metric(df[\"Temp\"], \"Temperature after scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80ee7f-4e1e-4c46-a506-dab8e49ae2c1",
   "metadata": {},
   "source": [
    "### 2.2 | Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d4e40-a096-4092-8664-679604492b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use the last 365 days as the test set\n",
    "end_date = df.index.max()\n",
    "start_date = end_date - pd.Timedelta(days=365)\n",
    "\n",
    "# Train/Test split\n",
    "train = df[df.index <= start_date]\n",
    "test = df.iloc[df.index > start_date] \n",
    "\n",
    "# Output\n",
    "print(f\"Training set size = {train.shape[0]} days\")\n",
    "print(f\"Testing set size = {test.shape[0]} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e67344-7f01-4f60-919d-48e180369312",
   "metadata": {},
   "source": [
    "### 2.3 | Converts our data to a format conducive to LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d6b40-1887-4f81-8c0b-9be27b4037ba",
   "metadata": {},
   "source": [
    "LSTMs take a sequence. In other words, we feed it X dates, and the LSTM will predict the (X+1) date. In order to feed the LSTM sequences, then, we need to use the \"TimeSeriesGenerator\" to convert our data into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3924f-1492-4416-b992-85dae745773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creates the training & testing set.\n",
    "n_input = 30\n",
    "generator_train = TimeseriesGenerator(data = train['Temp'].values.reshape(-1, 1), targets = train['Temp'].values ,\n",
    "                                      length = n_input, batch_size=32)\n",
    "generator_test = TimeseriesGenerator(data = test['Temp'].values.reshape(-1, 1), targets = test['Temp'].values ,\n",
    "                                      length = n_input, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514913e-4c62-475d-bc37-166697ca181f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displays the raw input & output\n",
    "X, y = generator_train[0]\n",
    "\n",
    "print(\"Here is how the neural network will work.\\n\")\n",
    "\n",
    "print(f'Given the Array: \\n{X[0]}')\n",
    "print(f'Predict this y: \\n {y[0]}')\n",
    "\n",
    "print(\"\\nKeep in mind these values are SCALED. Here's what the unscaled looks like\")\n",
    "print(\"Here's the X: \")\n",
    "print(scaler.inverse_transform( X[0] ) )\n",
    "print(\"Here's the y (next day's temperature):\")\n",
    "print(scaler.inverse_transform( y[0].reshape(-1, 1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c6d2c-ab6e-4d9f-b6b7-77e2ea871b71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3 | LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad3867-65af-49d5-8be4-fe41061255ad",
   "metadata": {},
   "source": [
    "### 3.0 | Section Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b868d64-4bfd-4f45-81dd-ebdc64cdacc8",
   "metadata": {},
   "source": [
    "You will create a neural network that uses LSTM cells in order to handle this time series data (§3.1) before plotting the output (§3.2).\n",
    "\n",
    "Here are a few parameters that may help you when creating an LSTM layer:\n",
    "<ul>\n",
    "  <li> <strong>activation.</strong> This is the activation function that's responsible for creating the new memory. This activation function occurs twice per cell: in the input gate (long term memory) and the output gate (short term memory). </li>\n",
    "  <li> <strong>recurrent activation.</strong> This is the activation function that's responsible for the percentage of memory to remember. This activation function occurs thrice per cell: in the forgotten gate (updating the long term memory), in the input gate (long term memory) and the output gate (short term memory). </li>\n",
    "    <li> <strong>return sequence</strong> By default, this is False. Keras sets up an LSTM layer such that the input passes through EACH NEURON in the layer. For example, if there are 64 neurons, the input will pass from LSTM cell #1, then to cell #2, etc. Sometimes, however, you want multiple LSTM layers running 'in parrallel' to each other. To implement this functionality, set the return_sequences = True until you get to the final LSTM layer. </li>\n",
    "</ul>\n",
    "\n",
    "Here is an LSTM cell that may help you better understand how the activation functions interact with the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a9846-e87d-4255-95a5-a2dada01d62c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/model_building/lstms_lstm_cell.jpg\" width=\"40%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef9d9c-8fa2-42ee-9414-2b2107857dff",
   "metadata": {},
   "source": [
    "<strong>The loss (error) on the testing set will be the primary means by which we evaluate your model.</strong>\n",
    "\n",
    "Do not modify the loss function. You must use mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c203b-75aa-4169-a403-d491f7b86585",
   "metadata": {},
   "source": [
    "### 3.1 | LSTM: Construct & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e59edf-642c-42f1-9b3d-d0d9c12ef4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model\n",
    "your_lstm_neural_network = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "your_lstm_neural_network.add( Input( shape= (n_input,1) ) )\n",
    "\n",
    "# Hidden (LSTM) Layers\n",
    "# Remember: you must specify the parameter \"return_sequences = True\" for all LSTM layers except the final LSTM layer\n",
    "your_lstm_neural_network.add(LSTM(1, activation=\"tanh\", recurrent_activation=\"sigmoid\", return_sequences = True)) \n",
    "your_lstm_neural_network.add(LSTM(1, activation=\"tanh\", recurrent_activation=\"sigmoid\",)) \n",
    "\n",
    "# Output Layer\n",
    "your_lstm_neural_network.add(Dense(1, activation = \"linear\", )) # Linear activation b/c we're predicting temperature (c.f., probability)\n",
    "\n",
    "# Compiles model\n",
    "your_lstm_neural_network.compile(loss='mse', optimizer=Adam(learning_rate=.001), \n",
    "             metrics=[metrics.MeanSquaredError(name='mse'),])\n",
    "your_lstm_neural_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37644d41-bac9-46e7-99e2-b1068d1818b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unlike previous exercises, you're free to modify the epochs.\n",
    "# Just be careful. Too many epochs might cost you a lot of time and not produce spectacular results.\n",
    "\n",
    "hist = your_lstm_neural_network.fit(generator_train, validation_data = generator_test, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624dfcd0-2d7c-4f95-9661-f289fdb8d930",
   "metadata": {},
   "source": [
    "### 3.2 | Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58d302-01bb-4abc-ba5a-191a014d7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Loss = {round (hist.history['loss'][-1], 8)}\")\n",
    "print(f\"Testing Loss  = {round(hist.history['val_loss'][-1], 8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9c96c-46df-4a06-91e5-18cdabeaed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, val_loss = hist.history[\"loss\"], hist.history[\"val_loss\"]\n",
    "plot_performance(loss, val_loss, \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad15cb-2257-4ae0-8205-1298f5846376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "_ = plot_time_series_results(your_lstm_neural_network, generator_train, scaler, train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d21097-143c-4f4e-ba29-51de80d28a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set\n",
    "_ = plot_time_series_results(your_lstm_neural_network, generator_test, scaler, test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1456b-d229-430a-825b-3a54e5e17428",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4 | Diagram Your Final LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2fdf0-9a10-4c05-a12d-911c94a6eadd",
   "metadata": {},
   "source": [
    "You can use the same tool as a feedforward neural network to diagram an LSTM neural network: https://alexlenail.me/NN-SVG/index.html\n",
    "\n",
    "When presenting your diagram, just be sure to explicitly identify any/all LSTM layers you used (as opposed to \"normal\" feedforward hidden layers)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
