{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc2e236-b4fe-4cb7-8b34-31743cf1d86e",
   "metadata": {},
   "source": [
    "# Mend the Money Marker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d34b8-896c-46c8-be3e-02e2d245987c",
   "metadata": {},
   "source": [
    "### Practicum Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5d13e-2f32-4d4e-aa90-1bc45ad7df94",
   "metadata": {},
   "source": [
    "In this practicum, we'll be \"mending\" the money marker. In other words, we're going to create a neural network that can identify individuals who, in 5 years time, will make more than $50,000. We'll use a real dataset from the 1994 US Census to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d349e-f0b2-44b0-9591-2206cc3c6cd2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/model_building/money_marker_icon.jpg\" width=\"25%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f7a67-4351-4e01-bcad-d75b6a106efc",
   "metadata": {},
   "source": [
    "However, there's an issue! Our first neural network sucks! It's diagramed below. The diagram is accurate except for the input layer. The actual input layer will have over 100 neurons (1 neuron per feature), but 100+ neurons cannot be intelligibly displayed in this sort of diagram. Thus, for the input layer, 1 neuron represents 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27bcad6-8ad2-4c00-8e69-e7891558df88",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/model_building/money_marker_initial_nn.jpg\" width=\"75%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c3e32-f3b2-4563-b0ef-c498e8055ca3",
   "metadata": {},
   "source": [
    "Thus, <strong> you will be tinkering with neural network architecture </strong> in order to fix this neural network. Namely, you'll be engaging in both steps of the neural network diagnosis prossess in which you'll combat (i) underfitting and then (ii) overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d50b32-f787-4706-a61a-96b9564d7c0e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/model_building/money_marker_steps_to_fixing_nn.jpg\" width=\"30%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2395090-93c5-42cf-b0e6-1ac77e94eac1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 0 | Google Colab Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f392ac5-5adf-4749-b7c8-61ec65491271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c809b6-7d80-4cf2-97e8-8b84efe2e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_safe(src, dst, max_len=200):\n",
    "    \"\"\"Copy files, skip long paths\"\"\"\n",
    "    skipped = 0\n",
    "    for root, dirs, files in os.walk(src):\n",
    "        rel_path = os.path.relpath(root, src)\n",
    "        dst_root = os.path.join(dst, rel_path) if rel_path != '.' else dst\n",
    "        if len(dst_root) < max_len:\n",
    "            os.makedirs(dst_root, exist_ok=True)\n",
    "            for file in files:\n",
    "                dst_file = os.path.join(dst_root, file)\n",
    "                if len(dst_file) < max_len:\n",
    "                    try: shutil.copy2(os.path.join(root, file), dst_file)\n",
    "                    except: skipped += 1\n",
    "                else: skipped += 1\n",
    "        else: skipped += len(files)\n",
    "    return skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0c834-a7a3-43f5-887c-3eee58993209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup resources if needed\n",
    "setup_ran = False\n",
    "if not os.path.exists('res'):\n",
    "    print(\"Setting up resources...\")\n",
    "    setup_ran = True\n",
    "    \n",
    "    # Cleanup, clone, copy\n",
    "    repo = 'deep_learning_resources'\n",
    "    if os.path.exists(repo):\n",
    "        shutil.rmtree(repo, onerror=lambda f,p,e: os.chmod(p, stat.S_IWRITE) or f(p))\n",
    "    \n",
    "    !git clone --depth=1 https://github.com/jjv31/deep_learning_resources\n",
    "    \n",
    "    if os.path.exists(f'{repo}/res'):\n",
    "        skipped = copy_safe(f'{repo}/res', 'res')\n",
    "        print(f\"Setup complete! {'(' + str(skipped) + ' long filenames skipped)' if skipped else ''}\")\n",
    "    \n",
    "    shutil.rmtree(repo, onerror=lambda f,p,e: os.chmod(p, stat.S_IWRITE) or f(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777b81c-cf38-4506-9dfd-2929e0fc774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only refresh if we just downloaded resources\n",
    "if setup_ran:\n",
    "    from IPython.display import Javascript, display\n",
    "    import time\n",
    "    \n",
    "    print(\"Refreshing images...\")\n",
    "    \n",
    "    # Try browser refresh + aggressive image reload\n",
    "    display(Javascript(f'''\n",
    "    try {{ setTimeout(() => window.location.reload(true), 2000); }} catch(e) {{}}\n",
    "    \n",
    "    const t = {int(time.time())};\n",
    "    document.querySelectorAll('img').forEach((img, i) => {{\n",
    "        if (img.src.includes('res/')) {{\n",
    "            const src = img.src.split('?')[0];\n",
    "            setTimeout(() => img.src = src + '?v=' + t + '_' + i, i * 50);\n",
    "        }}\n",
    "    }});\n",
    "    '''))\n",
    "    \n",
    "    print(\"If images don't appear, press Ctrl+Shift+R to hard refresh!\")\n",
    "else:\n",
    "    print(\"Resources already exist, skipping setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c8642-35c9-4480-879f-009355b72a4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1 | Imports & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d861f2-77f8-401b-b33c-01954c4ad5cc",
   "metadata": {},
   "source": [
    "### 1.0 | Imports & Auxilary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bdd2e9-22cb-42c6-b3db-969d7de66cd0",
   "metadata": {},
   "source": [
    "Just run these. No need to modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Scikit-learn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Set plot styles\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "# Get Pandas to display all rows/columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac113098-1d19-43cf-8f24-1a089f4d3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutes Pandas' annoying future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcce4d-4a1f-4124-bd0b-f2a0b17dd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c437a-5fed-44df-9c88-64494a3aedc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_univariates_metric(data, nameToPrint=None):\n",
    "\n",
    "    # Mode - Handling multimodal cases\n",
    "    mode_result = data.mode()\n",
    "    if len(mode_result) == 0:  # No mode found\n",
    "        mode_result = None\n",
    "    else:\n",
    "        mode_result = mode_result[0]\n",
    "\n",
    "    # Print output\n",
    "    print(f\"Descriptives for {nameToPrint}\")\n",
    "    print(f\"Mean = {round(data.mean(),2)} | Median = {round(data.median(),2)} | Mode = {mode_result} | \"\n",
    "          f\"Min = {data.min()} | Max = {data.max()} | SD = {round(data.std(),2)} | \"\n",
    "          f\"IQR(25) = {data.quantile(0.25)} | IQR(75) = {data.quantile(0.75)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ea3f8-cc84-4bff-a145-31bd1452e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to facilitate evaluating our models\n",
    "def print_score(clf, X, y_true):\n",
    "\n",
    "    # Gets predicted labels\n",
    "    if isinstance(clf, keras.models.Sequential): # If the model is a Keras neural network\n",
    "        y_pred = (clf.predict(X) >= 0.5).astype(int) \n",
    "    else: # Normal scikit-learn model\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "    # Gets key performance indicators\n",
    "    accuracy = round(accuracy_score(y_true, y_pred), 4)\n",
    "    recall = round(recall_score(y_true, y_pred), 4)\n",
    "    precision = round(precision_score(y_true, y_pred), 4)\n",
    "    f1 = round(f1_score(y_true, y_pred), 4)\n",
    "\n",
    "    # Displays them\n",
    "    print(f\"F1 = {f1:.4f} | Recall = {recall* 100:.2f}% | Precision = {precision*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc01e49-714f-46e2-8c60-2a6ede852838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the performance of the neural network\n",
    "def plot_performance(training_values, validation_values, metric_name = \"Recall\"):\n",
    "\n",
    "    epochs = range(1, len(training_values) + 1)\n",
    "    \n",
    "    sns.set() \n",
    "    plt.plot(epochs, training_values, '-', label=f'Training {metric_name}')\n",
    "    plt.plot(epochs, validation_values, ':', label=f'Validation {metric_name}')\n",
    "\n",
    "    plt.title(f'Training and Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9e797-92e9-4a90-a958-cc89d1b66469",
   "metadata": {},
   "source": [
    "### 1.1 | Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv(\"res/model_building/income.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the label (income_5y)\n",
    "df[\"income_5y\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b3006-715d-4e0b-8cc0-8ab2d7ca4627",
   "metadata": {},
   "source": [
    "### 1.2 | Preprocessing: Check for NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e213b-4969-40a5-8b9d-ad42317993e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks to see if there's any NAs\n",
    "assert(df.isnull().sum().all() == 0)\n",
    "print(\"Congratulations. There are no NAs in your dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7c13b-e06c-4abb-bdcc-0ace2bf32220",
   "metadata": {},
   "source": [
    "### 1.3 | Preprocessing: Handle Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43197d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts age to z-scores, as neural networks prefer smaller numbers \n",
    "print_univariates_metric(df[\"age\"], \"Age\")\n",
    "\n",
    "u = df[\"age\"].mean()\n",
    "sd = df[\"age\"].std()\n",
    "\n",
    "# Converts to z-scores\n",
    "df[\"age\"] = round( ( (df[\"age\"] - u) / sd ), 4)\n",
    "print_univariates_metric(df[\"age\"], \"Age (z-scores)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4007cf-4521-44e4-872a-0e8aa80c1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts hours per week to z-scores\n",
    "print_univariates_metric(df[\"hours-per-week\"], \"Hours per Week\")\n",
    "\n",
    "u = df[\"hours-per-week\"].mean()\n",
    "sd = df[\"hours-per-week\"].std()\n",
    "\n",
    "# Converts to z-scores\n",
    "df[\"hours-per-week\"] = round( ( (df[\"hours-per-week\"] - u) / sd ), 4)\n",
    "print_univariates_metric(df[\"hours-per-week\"], \"Hours per Week (z-scores)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0601d3a-17ca-40ba-8006-40d63a4be503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLES CAPITAL GAIN\n",
    "\n",
    "# Prints capital gain \n",
    "print_univariates_metric(df[\"capital-gain\"], \"Capital Gain\")\n",
    "print_univariates_metric(df[\"capital-gain\"][df[\"capital-gain\"] != 0], \"Capital Gain (no zeros)\")\n",
    "# Most don't have capital gain. Of those who do, it's usually modest (<$10K).\n",
    "\n",
    "# Let's convert this large number into binaries that summarize it\n",
    "df[\"has_capital_gain\"] = df[\"capital-gain\"] > 0\n",
    "df[\"capital_gain_above_median\"] = df[\"capital-gain\"] > np.median( df[\"capital-gain\"][df[\"capital-gain\"] != 0] )\n",
    "df[\"capital_gain_above_mean\"] = df[\"capital-gain\"] > np.mean( df[\"capital-gain\"][df[\"capital-gain\"] != 0] )\n",
    "\n",
    "# Now that we've effectively summarized capital gain, we can drop it.\n",
    "df = df.drop(columns=[\"capital-gain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d7d84-1f0e-46f0-9e87-cc8b24b3fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLES CAPITAL LOSS\n",
    "\n",
    "# Prints capital gain \n",
    "print_univariates_metric(df[\"capital-loss\"], \"Capital Loss\")\n",
    "print_univariates_metric(df[\"capital-loss\"][df[\"capital-loss\"] != 0], \"Capital loss (no zeros)\")\n",
    "\n",
    "# Let's convert this large number into binaries that summarize it.\n",
    "# Mean and median are about the same, so we'll just stick with mean\n",
    "df[\"has_capital_loss\"] = df[\"capital-loss\"] > 0\n",
    "df[\"capital_loss_above_mean\"] = df[\"capital-loss\"] > np.mean( df[\"capital-loss\"][df[\"capital-loss\"] != 0] )\n",
    "\n",
    "# Now that we've effectively summarized capital gain, we can drop it.\n",
    "df = df.drop(columns=[\"capital-loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5348f-87d8-4f71-b902-ac9c7750c4eb",
   "metadata": {},
   "source": [
    "### 1.4 | Preprocessing: Handle Nominal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500da62-35da-42b3-a49c-000dfb44a4aa",
   "metadata": {},
   "source": [
    "We'll handle these by one-hot encoding them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0e674-cea3-4f0d-8b3c-e93cdafd47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea1b74-fffd-46cd-aaae-3947e88a4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['workclass', 'education', \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"],\n",
    "                    prefix=['class_of_work', 'education', \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6897f0-ad9f-42ea-8a89-86f109c94436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converts all bools to binary integers (0/1)\n",
    "# Ignore the warnings\n",
    "boolean_cols = df.select_dtypes(include='bool').columns\n",
    "df.loc[:, boolean_cols] = df[boolean_cols].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ddfab6-e730-4c05-8920-a3693d74d3d5",
   "metadata": {},
   "source": [
    "### 1.5 | View Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c3991-46eb-4599-bdbc-004297e6c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3560a-dffe-493d-9a19-616801d91f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e11fc0b-7464-4d77-8ad2-5787d009ea65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2 | Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the output (y) from the inputs (X). The output is what we're hoping to predict.\n",
    "# In machine learning lingo, the input variable should be named X (capital x) and the output variable should be named y (lowercase y)\n",
    "X = df.drop(['income_5y'], axis=1)\n",
    "y = df['income_5y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays first rows of features. Confirms (a) they do not contain the label and (b) they look OK\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33732a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays first 3 outputs. Confirms we only have the label (i.e. whether an individual will commit GH)\n",
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's examine the sizes of the training and testing sets\n",
    "print(f\"Training set size = {y_train.size}\\nTesting set size = {y_test.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f82e803-c7ed-42db-a1aa-7c270015fbf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3 | Create Initial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d152aeb-e17e-4e57-abcb-c52fcdb15c69",
   "metadata": {},
   "source": [
    "### 3.0 | Section Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef504e1-3d53-42ad-9d71-1d93b129b4b1",
   "metadata": {},
   "source": [
    "We'll construct & train the initial neural network (ยง3.1), evaluate it so that it outputs a score (ยง3.2), and then evaluate it's training (ยง3.3). In the next section (ยง4), you'll go through this model and fix it.\n",
    "\n",
    "However, there's a rule --- <strong> do NOT change the number of epoches or the batch size </strong>. In other words, we're going to make it so you can only train the neural network on 50 iterations of the training set. \n",
    "\n",
    "This means you will be forced to improve the neural network by tinkering with its architecture. Allowing it to train more (via increasing the number of epoches) will certainly improve its performance, but that would be antithetical to the learning objective of understanding neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4425b0b-809a-4ffb-ad3e-80c804d8bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize constants. DO NOT CHANGE ANY OF THESE.\n",
    "EPOCHES = 50\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "NUMBER_OF_FEATURES = X.shape[1]\n",
    "print(f\"There are {NUMBER_OF_FEATURES} to be inputted into the neural network. Thus, there should be {NUMBER_OF_FEATURES} input nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf3ef3-0b4c-44e8-85bf-284d2088d092",
   "metadata": {},
   "source": [
    "### 3.1 | Construct & Train Initial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ed4a0-3b36-46e0-9ec4-dabb2e0fbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network\n",
    "initial_neural_network = Sequential() \n",
    "initial_neural_network.add( Input( shape= (NUMBER_OF_FEATURES,) ) ) \n",
    "initial_neural_network.add(Dense(1, activation='linear'))\n",
    "initial_neural_network.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# Compiles the model\n",
    "my_learning_rate = 0.000000001\n",
    "initial_neural_network.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=my_learning_rate), \n",
    "              metrics=[keras.metrics.Precision(name=\"precision\"), keras.metrics.Recall(name=\"recall\"),]) \n",
    "initial_neural_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec1ece-73b5-4cf5-89fa-04c915506424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trains the model\n",
    "hist = initial_neural_network.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                 epochs=EPOCHES, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369cd32-2ac9-4bf1-a421-acf9e9d1408f",
   "metadata": {},
   "source": [
    "### 3.2 | Evaluates Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb47f7-c8d4-47ab-9aac-e6c36f5264ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(initial_neural_network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d2ee9-b8c9-4cf3-8510-b8b9d90b6608",
   "metadata": {},
   "source": [
    "### 3.3 | Evaluates Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90e198-a1c2-4faf-bf56-bd73f714f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, val_loss = hist.history[\"loss\"], hist.history[\"val_loss\"]\n",
    "plot_performance(loss, val_loss, \"Loss (Error)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f060f-a09f-45c6-8bac-fa8b54247859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prec, val_prec = hist.history[\"precision\"], hist.history[\"val_precision\"]\n",
    "plot_performance(prec, val_prec, \"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc0a64-acc5-44ec-ae37-773555b4de45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recall, val_recall = hist.history[\"recall\"], hist.history[\"val_recall\"]\n",
    "plot_performance(recall, val_recall, \"Recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52abbe9-6c35-459e-abaa-3cabe64e8ae2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4 | Combat Underfitting: Fix the Neural Network (\"Money Marker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd920633-1271-4b84-bdb7-068b557486f4",
   "metadata": {},
   "source": [
    "### 4.0 | Section Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc4a733-6ca8-41a5-977c-30a4b1a53b43",
   "metadata": {},
   "source": [
    "Our neural network is so bad that it's not even fitting the training data. Thus, the first step should be to combat underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eadca2c-da22-497c-b0c9-604c464ce7e4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/model_building/money_marker_steps_to_fixing_nn_underfitting.jpg\" width=\"30%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0db8e7-63a7-4102-975d-f8adefdca240",
   "metadata": {},
   "source": [
    "You're welcome to perform one or more of the following operations to combat underfitting:\n",
    "\n",
    "<ul>\n",
    "  <li> Add or remove more neurons to the hidden layer. </li>\n",
    "  <li> Add more hidden layer(s). </li>\n",
    "  <li> Adjust the activation function. </li>\n",
    "  <li> Adjust the learning rate. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d3c712-66b6-4f9a-b740-761ce8a678dd",
   "metadata": {},
   "source": [
    "### 4.1 | Create your Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d93ce-b8c9-40d3-a464-e4f700f35441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************\n",
    "# EXERCSE\n",
    "# *******************************\n",
    "\n",
    "# Fix this neural network\n",
    "\n",
    "# Create neural network\n",
    "your_neural_network = Sequential() \n",
    "your_neural_network.add( Input( shape= (NUMBER_OF_FEATURES,) ) ) \n",
    "your_neural_network.add(Dense(1, activation='linear'))\n",
    "your_neural_network.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# Compiles the model\n",
    "my_learning_rate = 0.00000001 # HINT: This is a very small learning rate. The default is 0.001.\n",
    "your_neural_network.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=my_learning_rate), \n",
    "              metrics=[keras.metrics.Precision(name=\"precision\"), keras.metrics.Recall(name=\"recall\"),]) \n",
    "your_neural_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618da7f-3615-4bef-8d77-3442f7fabaae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trains the model.\n",
    "hist = your_neural_network.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHES, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f1130-c429-4a5f-a2cd-2447ca7d10fd",
   "metadata": {},
   "source": [
    "### 4.2 | Evaluates Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef93fc-cc89-423c-b627-ff8d1ccc503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Model\n",
    "print(\"Score of your model.\")\n",
    "print_score(your_neural_network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90751ea-7682-41a9-82ff-d98e3475e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here's the score of the initial neural network. Does your F1 score exceed this one?\")\n",
    "print_score(initial_neural_network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ee157-80d2-4645-9b4e-48add52dd051",
   "metadata": {},
   "source": [
    "### 4.3 | Evaluates Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40219f-985e-48c1-b42a-dcf72246bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, val_loss = hist.history[\"loss\"], hist.history[\"val_loss\"]\n",
    "plot_performance(loss, val_loss, \"Loss (Error)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41431c-076e-4673-9052-a63b4ac9c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, val_prec = hist.history[\"precision\"], hist.history[\"val_precision\"]\n",
    "plot_performance(prec, val_prec, \"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb0191-f09d-42fe-9499-b8fa13cccf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, val_recall = hist.history[\"recall\"], hist.history[\"val_recall\"]\n",
    "plot_performance(recall, val_recall, \"Recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcfb0d-3b2b-4b37-9da8-cc33cbbe29f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5 | Combat Overfitting: Fix the Neural Network (\"Money Marker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d673a143-327d-4a0c-bf83-bb3538c950da",
   "metadata": {},
   "source": [
    "### 5.0 | Section Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bcf469-e37c-490b-90d2-42000dfd9514",
   "metadata": {},
   "source": [
    "Good news! I managed to create a neural network that works! The bad news is that it's overfitted, something we'll need to fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa8e5d0-2022-43fe-87c5-30b7cc272cae",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/model_building/money_marker_steps_to_fixing_nn_overfitting.jpg\" width=\"30%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7d731-a3d4-42c1-ad48-3b2d9bd7e3d2",
   "metadata": {},
   "source": [
    "To combat overfitting, you will do one of the following:\n",
    "\n",
    "<ul>\n",
    "  <li> Add L1 or L2 Regularization. </li>\n",
    "  <li> Add a dropout layer </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa7cbb-e31d-4736-a8d4-98978f2c14c2",
   "metadata": {},
   "source": [
    "### 5.1 | Create and Display Overfitted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63deab5-4d5b-47d5-95ec-c8156b77c251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's create an overfitted neural network.\n",
    "overfitted_neural_network = Sequential() \n",
    "overfitted_neural_network.add( Input( shape= (NUMBER_OF_FEATURES,) ) ) \n",
    "overfitted_neural_network.add(Dense(64, activation='relu'))\n",
    "overfitted_neural_network.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# Compiles the model\n",
    "my_learning_rate = 0.01\n",
    "overfitted_neural_network.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=my_learning_rate), \n",
    "              metrics=[keras.metrics.Precision(name=\"precision\"), keras.metrics.Recall(name=\"recall\"),]) \n",
    "\n",
    "# Trains the overfitted model\n",
    "hist = overfitted_neural_network.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHES, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c72df-1525-43b4-a969-faf4faa9bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the scores\n",
    "print(\"Score of overfitted model\")\n",
    "print_score(overfitted_neural_network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26080d8c-6a51-4082-b6b8-89588228adc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The score isn't bad, but let's look at the loss function.\n",
    "# Validation loss should go down with training loss, but it's not. That's evidence of overfitting.\n",
    "\n",
    "loss, val_loss = hist.history[\"loss\"], hist.history[\"val_loss\"]\n",
    "plot_performance(loss, val_loss, \"Loss (Error)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a4808-e60d-46ad-b9a8-1bb19b2d5c7d",
   "metadata": {},
   "source": [
    "### 5.2 | Your Fix: Compile & Train Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c702d6-e1bb-4da7-93f5-28a178e7f44e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ***************************\n",
    "# EXERCISE \n",
    "# ***************************\n",
    "\n",
    "# The overfitted model is pasted below. \n",
    "# Change the model by either adding L1/L2 regularization OR a dropout layer.\n",
    "\n",
    "# To add a dropout layer\n",
    "# your_fix.add(Dropout(0.XX)) \n",
    "# Replace 0.XX with a decimal between 0 and 1. This is the percentage of neurons in the previous layer that should be deleted.\n",
    "\n",
    "# To add regularization, add it to \n",
    "# your_fix.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001) ) )\n",
    "# \" kernel_regularizer=regularizers.l2(0.001) \" is the regularization technique you're adding\n",
    "# l1 regularization uses absolute value, whereas l2 regularization uses the square of the weights.\n",
    "# The number is the penalty term, where the higher the term, the greater the regularization\n",
    "\n",
    "your_fix = Sequential() \n",
    "your_fix.add( Input( shape= (NUMBER_OF_FEATURES,) ) ) \n",
    "your_fix.add(Dense(64, activation='relu') )\n",
    "your_fix.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# Compiles the model\n",
    "my_learning_rate = 0.01\n",
    "your_fix.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=my_learning_rate), \n",
    "              metrics=[keras.metrics.Precision(name=\"precision\"), keras.metrics.Recall(name=\"recall\"),]) \n",
    "your_fix.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce903ba4-60ef-4955-8df4-ada8dfca2300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trains your model\n",
    "hist = your_fix.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHES, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31802f6c-85b4-4a19-b380-aa71026eb3e5",
   "metadata": {},
   "source": [
    "### 5.3 | Evaluate Your Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb3de8-4e24-4f9b-8a3c-d7e3d81cd79c",
   "metadata": {},
   "source": [
    "Let's start by graphing the loss function. Does the validation loss overlap with the training loss? If so, congratulations! You've adjusted overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0240bf-571d-4f7e-b4e8-f75fea4b3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, val_loss = hist.history[\"loss\"], hist.history[\"val_loss\"]\n",
    "plot_performance(loss, val_loss, \"Loss (Error)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15a855-6c47-499e-9219-bc2eacd8e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the scores\n",
    "print(\"Score of your model.\")\n",
    "print_score(your_fix, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb6874-705d-4450-96c8-ccaf1938cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the scores\n",
    "print(\"Score of the original overfitted model.\")\n",
    "print_score(overfitted_neural_network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdeee3a-f72a-4c5d-be37-142ac2f346d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
