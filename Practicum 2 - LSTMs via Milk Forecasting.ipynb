{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94c25c5-320d-443e-80bc-95da0afa7b2a",
   "metadata": {},
   "source": [
    "# Using LSTMs to Forecast Monthly Milk Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c864f-5f73-4619-86e2-cff92fb113fd",
   "metadata": {},
   "source": [
    "Time series datasets are defined as databases that contain a sequence of datapoints over time. This includes stock prices (e.g., price per day), weather (e.g., degrees Celsius per day), and sales figures (net profit by quarter), among others.\n",
    "\n",
    "In this example, we'll use monthly milk production, which is stored as units of milk produced per month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dafafe-31be-41e1-ba6d-4050f78fad67",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/model_building/lstms_milk_icon.jpg\" width=\"25%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478f4c1-3dc3-404b-9c79-be87176752d7",
   "metadata": {},
   "source": [
    "Unfortunately, neural networks have a difficult time with memory and variable input features. This was fixed by recurrent neural networks and later by long short-term memory networks (LSTMs). This makes them ideal for forecasting with time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e346686-1a58-4780-8389-5fb79f9ebe8e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/model_building/lstms_lstm_cell.jpg\" width=\"50%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c277da-620b-430a-99dc-5757acf9181b",
   "metadata": {},
   "source": [
    "In this practicum, we'll be using LSTMs to predict monthly milk production. <strong> We'll be using a very high level implementation of LSTMs, thus, you won't need to know the details of how an LSTM cell works.</storng>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e404adb-45e6-4f9c-9b36-ef2128b84c00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 0 | Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9d78f-b696-45e2-aa95-e2281b4547a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f2093-b4b6-495d-a503-429d027d3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_safe(src, dst, max_len=200):\n",
    "    \"\"\"Copy files, skip long paths\"\"\"\n",
    "    skipped = 0\n",
    "    for root, dirs, files in os.walk(src):\n",
    "        rel_path = os.path.relpath(root, src)\n",
    "        dst_root = os.path.join(dst, rel_path) if rel_path != '.' else dst\n",
    "        if len(dst_root) < max_len:\n",
    "            os.makedirs(dst_root, exist_ok=True)\n",
    "            for file in files:\n",
    "                dst_file = os.path.join(dst_root, file)\n",
    "                if len(dst_file) < max_len:\n",
    "                    try: shutil.copy2(os.path.join(root, file), dst_file)\n",
    "                    except: skipped += 1\n",
    "                else: skipped += 1\n",
    "        else: skipped += len(files)\n",
    "    return skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be793fa-c830-4c06-a1a0-ec60b6f00dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup resources if needed\n",
    "setup_ran = False\n",
    "if not os.path.exists('res'):\n",
    "    print(\"Setting up resources...\")\n",
    "    setup_ran = True\n",
    "    \n",
    "    # Cleanup, clone, copy\n",
    "    repo = 'deep_learning_resources'\n",
    "    if os.path.exists(repo):\n",
    "        shutil.rmtree(repo, onerror=lambda f,p,e: os.chmod(p, stat.S_IWRITE) or f(p))\n",
    "    \n",
    "    !git clone --depth=1 https://github.com/jjv31/deep_learning_resources\n",
    "    \n",
    "    if os.path.exists(f'{repo}/res'):\n",
    "        skipped = copy_safe(f'{repo}/res', 'res')\n",
    "        print(f\"Setup complete! {'(' + str(skipped) + ' long filenames skipped)' if skipped else ''}\")\n",
    "    \n",
    "    shutil.rmtree(repo, onerror=lambda f,p,e: os.chmod(p, stat.S_IWRITE) or f(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89eb0e-5773-496d-880b-d7b0bc4c86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only refresh if we just downloaded resources\n",
    "if setup_ran:\n",
    "    from IPython.display import Javascript, display\n",
    "    import time\n",
    "    \n",
    "    print(\"Refreshing images...\")\n",
    "    \n",
    "    # Try browser refresh + aggressive image reload\n",
    "    display(Javascript(f'''\n",
    "    try {{ setTimeout(() => window.location.reload(true), 2000); }} catch(e) {{}}\n",
    "    \n",
    "    const t = {int(time.time())};\n",
    "    document.querySelectorAll('img').forEach((img, i) => {{\n",
    "        if (img.src.includes('res/')) {{\n",
    "            const src = img.src.split('?')[0];\n",
    "            setTimeout(() => img.src = src + '?v=' + t + '_' + i, i * 50);\n",
    "        }}\n",
    "    }});\n",
    "    '''))\n",
    "    \n",
    "    print(\"If images don't appear, press Ctrl+Shift+R to hard refresh!\")\n",
    "else:\n",
    "    print(\"Resources already exist, skipping setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ff39f-2fa1-4f1f-b925-ac2c9aae92ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1 | Loads & Inspects Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e262e1-b625-4c72-8f2b-57cdfbb0da98",
   "metadata": {},
   "source": [
    "### 1.0 | Install missing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c12ed-59aa-4eb8-8bdc-fe9d35cd62fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install statsmodels pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259ad2c-05ef-4b78-b810-a5bbf377a9a5",
   "metadata": {},
   "source": [
    "### 1.1 | Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdcdfe6-35b4-4bac-98fe-c56256eb91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Other\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Neural Nets\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a06c6-70aa-4d66-9d21-c0c699bc2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f622802-2304-45c6-add9-e4154367e06a",
   "metadata": {},
   "source": [
    "### 1.2 | Aux functions. Just run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b78be0-4015-4f39-bd3e-16b3ba28f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to facilitate evaluating our models\n",
    "def print_score(clf, X, y_true):\n",
    "\n",
    "    # Gets predicted labels\n",
    "    if isinstance(clf, keras.models.Sequential): # If the model is a Keras neural network\n",
    "        y_pred = (clf.predict(X) >= 0.5).astype(int) \n",
    "    else: # Normal scikit-learn model\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "    # Gets key performance indicators\n",
    "    accuracy = round(accuracy_score(y_true, y_pred), 4)\n",
    "    recall = round(recall_score(y_true, y_pred), 4)\n",
    "    precision = round(precision_score(y_true, y_pred), 4)\n",
    "    f1 = round(f1_score(y_true, y_pred), 4)\n",
    "\n",
    "    # Displays them\n",
    "    print(f\"F1 = {f1:.4f} | Recall = {recall* 100:.2f}% | Precision = {precision*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c274b7c-8d2e-4bf3-b3de-939ee1df2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the performance of the neural network\n",
    "def plot_performance(training_values, validation_values, metric_name = \"Recall\"):\n",
    "\n",
    "    epochs = range(1, len(training_values) + 1)\n",
    "    \n",
    "    sns.set() \n",
    "    plt.plot(epochs, training_values, '-', label=f'Training {metric_name}')\n",
    "    plt.plot(epochs, validation_values, ':', label=f'Validation {metric_name}')\n",
    "\n",
    "    plt.title(f'Training and Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724076a3-8bb9-4325-a98f-1f9e7e580273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the results of a time series.\n",
    "# model is the neural network\n",
    "# generator_to_evaluate is either the generator trained on your training or testing set\n",
    "# scaler is the min-max scaler trained in ยง2.\n",
    "def plot_time_series_results(model, generator_to_evaluate, scaler, dates_index):\n",
    "\n",
    "    # Returns pandas dataframe that contains (i) actual value (milk production) and (ii) predicted value. Both descaled.\n",
    "    def get_results_df():\n",
    "        \n",
    "        # Creates list to store  (i) predicted and (ii) actual values\n",
    "        all_predictions = []\n",
    "        all_actuals = []\n",
    "\n",
    "        for i in range(len(generator_to_evaluate)):\n",
    "            # Gets a batch of (i) X features and (i) the target they're trying to predict\n",
    "            x_batch, y_batch = generator_to_evaluate[i]\n",
    "\n",
    "            # Make predictions on the current batch\n",
    "            batch_predictions = model.predict(x_batch, verbose=0)\n",
    "\n",
    "            # Extend our lists with the current batch's predictions and actuals\n",
    "            # Flatten them if they are in shape (batch_size, 1) to (batch_size,)\n",
    "            all_predictions.extend(batch_predictions.flatten())\n",
    "            all_actuals.extend(y_batch.flatten())\n",
    "\n",
    "        # Convert lists to NumPy arrays for easier manipulation\n",
    "        all_predictions = np.array(all_predictions).reshape(-1, 1) # Reshape back to (n_samples, 1) for inverse_transform\n",
    "        all_actuals = np.array(all_actuals).reshape(-1, 1)\n",
    "\n",
    "        # Descales predictions (via the scaler) so they're intelligible again (i.e., not approximately 0-1)\n",
    "        all_predictions = scaler.inverse_transform(all_predictions)\n",
    "        all_actuals = scaler.inverse_transform(all_actuals)\n",
    "\n",
    "\n",
    "        # Create a DataFrame for easy viewing\n",
    "        results_df = pd.DataFrame({'Actual': all_actuals.flatten(), 'Predicted': all_predictions.flatten()},\n",
    "                                 index = dates_index[n_input:])\n",
    "        return results_df\n",
    "\n",
    "    # Plots the results df. Takes the results_df returned in the above subfunction.\n",
    "    def plot_results_df(results_df):\n",
    "\n",
    "        # Defines plot size\n",
    "        plt.figure(figsize=(7, 5))\n",
    "\n",
    "        # Plots vals\n",
    "        plt.plot(results_df['Actual'], label='Actual Values (Test Set)', color='blue', linewidth=2, marker='o', markersize=4)\n",
    "        plt.plot(results_df['Predicted'], label='Predicted Values (Test Set)', color='red', linestyle='--', linewidth=1.5, marker='o', markersize=4)\n",
    "\n",
    "        \n",
    "        # Labels\n",
    "        plt.title('LSTM Model Predictions vs. Actuals')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Mikl Production (Original Scale)')\n",
    "\n",
    "        # Other\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # Main functions\n",
    "    results_df = get_results_df()\n",
    "    plot_results_df(results_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d62b9-73f2-49eb-8f45-e706b52a723e",
   "metadata": {},
   "source": [
    "### 1.3 | Loads & Inspects Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fd8a0-d8c0-4bb1-8b9e-dd2aee6216aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load the data using the pandas dataframe. We will use milk production data\n",
    "df = pd.read_csv('res/lstm/monthly_milk_production.csv',\n",
    "                 index_col='Date',\n",
    "                 parse_dates=True)\n",
    "df.index.freq = 'MS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc5aee-3ffe-400e-a995-acfde4c8f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspects dataset\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5f3ae-953b-4cb0-abe0-73f2bc2451f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting graph: production and date\n",
    "df.plot(figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d843f-7b9e-4a18-84cc-37f598a4b1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Same as above, except it plots seasonality, trends, and noise.\n",
    "# Noise is defined as time series datapoint - trend - seasonality\n",
    "seasonal_decompose(df['Production']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ab85f-9f87-482c-a0dc-6e79f4e281de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2 | Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f91d1-f98f-465d-a3c9-5f509abba262",
   "metadata": {},
   "source": [
    "### 2.1 | Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d4e40-a096-4092-8664-679604492b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing.\n",
    "# We're not using scikit learn's train/test split becuase we want the most recent years to be forecasted\n",
    "\n",
    "train = df.iloc[:156] \n",
    "test = df.iloc[156:] \n",
    "\n",
    "print(f\"Training set size: {train.shape[0]} months\")\n",
    "print(f\"Training set size: {test.shape[0]} months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86643a-16fe-4ebb-ad64-80cad7105730",
   "metadata": {},
   "source": [
    "### 2.2 | Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a029e0-cea9-4f07-aa08-a4313a2b686a",
   "metadata": {},
   "source": [
    "Neural networks are extremely sensitive to large values. We'll need to convert the numeric features (milk production) into something a neural network can handle, like a 0-1 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27651e-edbf-4fe5-bcb6-417ae21d1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These numbers (milk production) are too large for a neural network to effectively handle.\n",
    "train[\"Production\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107b605-ab8e-46aa-ab69-12931c0b44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a min-max scaler, in which all values are scaled between the minimum value and the max value\n",
    "# See here for formula: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3414e32-8d5d-4844-a580-e6b1ae781236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the scaler's parameters on the training data (e.g., x min, x max)\n",
    "scaler.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1acfa6a-775d-498f-9d6b-cddc076685b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scales the training & test data\n",
    "scaled_train = scaler.transform(train)\n",
    "scaled_test = scaler.transform(test)\n",
    "\n",
    "# Ensures scaler worked\n",
    "print(f\"min milk value before scaling = {min(train['Production'])} | with scaling = {min(scaled_train)}\")\n",
    "print(f\"max milk value after scaling = {max(train['Production'])} | with scaling = {max(scaled_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d1d81-3f79-411f-a0d4-bf481c118787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints results\n",
    "print(\"First three scaled values\")\n",
    "print(scaled_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e67344-7f01-4f60-919d-48e180369312",
   "metadata": {},
   "source": [
    "### 2.3 | Converts our data to a format conducive to LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d6b40-1887-4f81-8c0b-9be27b4037ba",
   "metadata": {},
   "source": [
    "LSTMs take a sequence. In other words, we feed it X dates, and the LSTM will predict the (X+1) date. In order to feed the LSTM sequences, then, we need to use the \"TimeSeriesGenerator\" to convert our data into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3924f-1492-4416-b992-85dae745773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creates the training & testing set.\n",
    "n_input = 3 # number of input months\n",
    "generator_train = TimeseriesGenerator(data = scaled_train, targets = scaled_train,\n",
    "                                      length = n_input, batch_size=32)\n",
    "generator_test = TimeseriesGenerator(data = scaled_test, targets = scaled_test,\n",
    "                                      length = n_input, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514913e-4c62-475d-bc37-166697ca181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the raw input & output\n",
    "X, y = generator_train[0]\n",
    "\n",
    "print(\"Here is how the neural network will work.\\n\")\n",
    "\n",
    "print(f'Given the Array: \\n{X[0]}')\n",
    "print(f'Predict this y: \\n {y[0]}')\n",
    "\n",
    "print(\"\\nKeep in mind these values are SCALED. Here's what the unscaled looks like\")\n",
    "print(\"Here's the X: \")\n",
    "print(scaler.inverse_transform(X[0]))\n",
    "print(\"Here's the y (the next month's milk production):\")\n",
    "print(scaler.inverse_transform([y[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c6d2c-ab6e-4d9f-b6b7-77e2ea871b71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3 | LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad3867-65af-49d5-8be4-fe41061255ad",
   "metadata": {},
   "source": [
    "### 3.0 | Section Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b868d64-4bfd-4f45-81dd-ebdc64cdacc8",
   "metadata": {},
   "source": [
    "We'll create a neural network of LSTM cells (ยง3.1) before plotting the output (ยง3.2) There's a few things to note. \n",
    "\n",
    "When creating the LSTM, Keras keeps its implementation to a very high level such that you don't need to know the specific inner-workings of a LSTM cell. However, there's a few parameters to note.\n",
    "<ul>\n",
    "  <li> <strong>activation.</strong> This is the activation function that's responsible for creating the new memory. This activation function occurs twice per cell: in the input gate (long term memory) and the output gate (short term memory). </li>\n",
    "  <li> <strong>recurrent activation.</strong> This is the activation function that's responsible for the percentage of memory to remember. This activation function occurs thrice per cell: in the forgotten gate (updating the long term memory), in the input gate (long term memory) and the output gate (short term memory). </li>\n",
    "    <li> <strong>return sequence</strong> By default, this is False. Keras sets up an LSTM layer such that the input passes through EACH NEURON in the layer. For example, if there are 64 neurons, the input will pass from LSTM cell #1, then to cell #2, etc. Sometimes, however, you want multiple LSTM layers running 'in parrallel' to each other. To implement this functionality, set the return_sequences = True until you get to the final LSTM layer. </li>\n",
    "</ul>\n",
    "\n",
    "When evaluating the model, please note this is not a classification problem. There are no 'neat' precision, recall, and f1 values. Instead, there's mean squared error, which is difficult to interpret. However, we can plot the predicted values alongside the actual values to assess the model's utility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c203b-75aa-4169-a403-d491f7b86585",
   "metadata": {},
   "source": [
    "### 3.1 | LSTM: Construct & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e59edf-642c-42f1-9b3d-d0d9c12ef4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model\n",
    "initial_lstm_neural_network = Sequential()\n",
    "initial_lstm_neural_network.add( Input( shape= (n_input,1) ) )  # Input layer. shape refers to (number_of_inputs, number_of_features)\n",
    "initial_lstm_neural_network.add(LSTM(128, activation=\"tanh\", recurrent_activation=\"sigmoid\", return_sequences = True)) \n",
    "initial_lstm_neural_network.add(LSTM(64, activation=\"tanh\", recurrent_activation=\"sigmoid\",)) \n",
    "initial_lstm_neural_network.add(Dense(1, activation = \"linear\", )) # Linear activation because our output is a number (c.f., sigmoid for binary yes/no)\n",
    "\n",
    "# Compiles model\n",
    "initial_lstm_neural_network.compile(loss='mse', optimizer=Adam(learning_rate=.001), \n",
    "             metrics=[metrics.MeanSquaredError(name='mse'),])\n",
    "initial_lstm_neural_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37644d41-bac9-46e7-99e2-b1068d1818b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = initial_lstm_neural_network.fit(generator_train, validation_data =generator_test, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624dfcd0-2d7c-4f95-9661-f289fdb8d930",
   "metadata": {},
   "source": [
    "### 3.2 | Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9c96c-46df-4a06-91e5-18cdabeaed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, val_loss = hist.history[\"loss\"], hist.history[\"val_loss\"]\n",
    "print(f\"Total Training Loss = {hist.history['loss'][-1]} | Testing Loss = {hist.history['val_loss'][-1]}\")\n",
    "plot_performance(loss, val_loss, \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad15cb-2257-4ae0-8205-1298f5846376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "_ = plot_time_series_results(initial_lstm_neural_network, generator_train, scaler, train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d21097-143c-4f4e-ba29-51de80d28a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set\n",
    "_ = plot_time_series_results(initial_lstm_neural_network, generator_test, scaler, test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a95c0-14f3-428d-9070-6e952f022d3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4 | Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd8124-1f18-4672-a39a-c293c45d209c",
   "metadata": {},
   "source": [
    "### 4.0 | Section Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc3743-45d4-4c02-9ade-b951c4e57891",
   "metadata": {},
   "source": [
    "Improve upon the previous LSTM. You can add more hidden layers, change the number of nodes, change the activation function to something like relu, etc.\n",
    "\n",
    "To determine if your model is better than the previous, your model should have a smaller loss function on the testing (validation) set.\n",
    "\n",
    "<strong> Do not change the loss function </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92fb8e-630c-4535-acbd-7879ca4c8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this method\n",
    "def did_you_win(your_loss, my_loss, silence = False):\n",
    "\n",
    "    # A neural network may perform better due to random chance. \n",
    "    # Thus, we'll use a 'tolerance' value. The model will not be considered better unless it exceeds the tolerance.\n",
    "    TOLERANCE = 0.01\n",
    "    you_lost = your_loss >= (my_loss-TOLERANCE)\n",
    "    \n",
    "    if you_lost:\n",
    "        print(f\"{'*'*20}\\nYou lost.\\n{'*'*20}\\nYour loss is greater than, equal to, or not significantly beter than my loss.\")\n",
    "        print(f\"Your neural network's loss must be {TOLERANCE} less than my neural network's loss to be considered better.\")\n",
    "        print(\"This is because your neural network may be doing better just due to random chance\")\n",
    "        print(\"This can be done by (i) improving fitting and (ii) combatting overfitting, if any exists.\")\n",
    "    else:\n",
    "        print(\"Congratulations! You won! Your model outperforms my model on the testing set!\")\n",
    "        \n",
    "    print(f\"\\nYour Loss = [{your_loss}] | My loss = {my_loss} | Loss Difference (Negative means your model does better) = {your_loss - my_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31791e4-fb39-449d-a480-afd966f16f21",
   "metadata": {},
   "source": [
    "### 4.1 | Create your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f02e12-0a4e-4958-b51e-30d2df91fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model\n",
    "your_model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "your_model.add( Input( shape= (n_input,1) ) ) \n",
    "\n",
    "# Hidden Layers\n",
    "your_model.add(LSTM(128, activation=\"tanh\", recurrent_activation=\"sigmoid\", return_sequences = True)) \n",
    "your_model.add(LSTM(64, activation=\"tanh\", recurrent_activation=\"sigmoid\",)) \n",
    "\n",
    "# Output Layer\n",
    "your_model.add(Dense(1, activation = \"linear\", ))\n",
    "\n",
    "# Compiles model\n",
    "your_model.compile(loss='mse', optimizer=Adam(learning_rate=.001), \n",
    "             metrics=[metrics.MeanSquaredError(name='mse'),])\n",
    "your_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460cba8e-ba98-46ac-ab71-8fb907a6a0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train your model\n",
    "your_hist = your_model.fit(generator_train, validation_data = generator_test, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6a68b-4516-49f0-b8fc-2f669d18fd5b",
   "metadata": {},
   "source": [
    "### 4.2 | Your Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5adb65-5ce4-4617-8a67-e7922e786337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints your results\n",
    "print(f\"Your Training Loss = {your_hist.history['loss'][-1]} | Your Testing Loss = {your_hist.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f059a-3bf4-4597-a71c-5c3ed23fe417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if you won. Just run this code. DO NOT MODIFY IT\n",
    "did_you_win(your_loss = your_hist.history['val_loss'][-1], my_loss = hist.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e8c98-a617-4109-b934-858896e94a69",
   "metadata": {},
   "source": [
    "### 4.3 | Your Graphs (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82788574-7fc7-4979-9cbb-ab3ffebe476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, val_loss = your_hist.history[\"loss\"], your_hist.history[\"val_loss\"]\n",
    "plot_performance(loss, val_loss, \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970aac3-baf9-46f2-981e-1a72643ee7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "_ = plot_time_series_results(your_model, generator_train, scaler, train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072380b-a5ac-4a6c-ab8d-9e7849f6ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set\n",
    "_ = plot_time_series_results(your_model, generator_test, scaler, test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76a25c-a835-4256-80d1-2eeba0e94403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
