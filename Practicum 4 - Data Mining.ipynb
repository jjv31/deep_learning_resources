{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2db5f47-5752-47db-8b88-05b901d44c2b",
   "metadata": {},
   "source": [
    "# Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1849db4-1fda-4236-88c6-61079878d91b",
   "metadata": {},
   "source": [
    "We'll be learning (i) the basics of web scraping and (ii) how to analyze unstructured text such that you can identify suicides on reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aacd67-a1aa-438c-a465-fa0397cb583c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/data_mining/reddit_suicides_logo.jpg\" width=\"20%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04347df9-cc24-4335-b28a-c6b39d56ffe9",
   "metadata": {},
   "source": [
    "To do this, we'll be undertaking web scraping & data cleaning (§1-3) before transforming the data (§4) and analyzing it via PCAs (§5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc8dc58a-3860-4fe3-811a-0bad87a4cf22",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <img src = \"res/data_mining/reddit_suicides_pipeline.jpg\" width=\"50%\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc8fa7-0637-4904-b9c2-669d02e8abe5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0 | Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14672b33-91ab-46a6-a053-3ed34bce1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35954b0c-d9a3-46cf-8e43-7762d7ff04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_safe(src, dst, max_len=200):\n",
    "    \"\"\"Copy files, skip long paths\"\"\"\n",
    "    skipped = 0\n",
    "    for root, dirs, files in os.walk(src):\n",
    "        rel_path = os.path.relpath(root, src)\n",
    "        dst_root = os.path.join(dst, rel_path) if rel_path != '.' else dst\n",
    "        if len(dst_root) < max_len:\n",
    "            os.makedirs(dst_root, exist_ok=True)\n",
    "            for file in files:\n",
    "                dst_file = os.path.join(dst_root, file)\n",
    "                if len(dst_file) < max_len:\n",
    "                    try: shutil.copy2(os.path.join(root, file), dst_file)\n",
    "                    except: skipped += 1\n",
    "                else: skipped += 1\n",
    "        else: skipped += len(files)\n",
    "    return skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c003941-8887-4b41-8b4b-f04227504e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup resources if needed\n",
    "setup_ran = False\n",
    "if not os.path.exists('res'):\n",
    "    print(\"Setting up resources...\")\n",
    "    setup_ran = True\n",
    "    \n",
    "    # Cleanup, clone, copy\n",
    "    repo = 'deep_learning_resources'\n",
    "    if os.path.exists(repo):\n",
    "        shutil.rmtree(repo, onerror=lambda f,p,e: os.chmod(p, stat.S_IWRITE) or f(p))\n",
    "    \n",
    "    !git clone --depth=1 https://github.com/jjv31/deep_learning_resources\n",
    "    \n",
    "    if os.path.exists(f'{repo}/res'):\n",
    "        skipped = copy_safe(f'{repo}/res', 'res')\n",
    "        print(f\"Setup complete! {'(' + str(skipped) + ' long filenames skipped)' if skipped else ''}\")\n",
    "    \n",
    "    shutil.rmtree(repo, onerror=lambda f,p,e: os.chmod(p, stat.S_IWRITE) or f(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61decb1f-9b9f-43a0-9335-bcee8b5f010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only refresh if we just downloaded resources\n",
    "if setup_ran:\n",
    "    from IPython.display import Javascript, display\n",
    "    import time\n",
    "    \n",
    "    print(\"Refreshing images...\")\n",
    "    \n",
    "    # Try browser refresh + aggressive image reload\n",
    "    display(Javascript(f'''\n",
    "    try {{ setTimeout(() => window.location.reload(true), 2000); }} catch(e) {{}}\n",
    "    \n",
    "    const t = {int(time.time())};\n",
    "    document.querySelectorAll('img').forEach((img, i) => {{\n",
    "        if (img.src.includes('res/')) {{\n",
    "            const src = img.src.split('?')[0];\n",
    "            setTimeout(() => img.src = src + '?v=' + t + '_' + i, i * 50);\n",
    "        }}\n",
    "    }});\n",
    "    '''))\n",
    "    \n",
    "    print(\"If images don't appear, press Ctrl+Shift+R to hard refresh!\")\n",
    "else:\n",
    "    print(\"Resources already exist, skipping setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000bdad-a3ec-411f-aaab-b496907663f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1 | Web Scraping Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc030ac-7e9f-4a1f-be89-deb54c22097a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Text pre-processing and visualization, respectively\n",
    "%pip install neattext wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed7015-6e0b-4195-ac12-9390b870245d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9844f057-e86e-4946-8394-4b11b35523f5",
   "metadata": {},
   "source": [
    "### 1.1 | Accessing our first webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b912b6-7785-4ea1-afa5-de6363dffad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for downloading webpages\n",
    "import requests # Main library for sending/recieving requests\n",
    "import urllib.request # For customizing requests (e.g., adding a header)\n",
    "from urllib.request import urljoin\n",
    "# Once we download our webpages, we need to make them legibile. We'll do this via BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Saving our webpages to the system\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633889c9-310d-4cbb-bab8-f70df42c3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.college.police.uk/app\"\n",
    "request = urllib.request.Request(url)\n",
    "request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1561057-beb9-4737-a815-ba00c94ba2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_webpage(request):\n",
    "    try:\n",
    "        with urllib.request.urlopen(request) as response:\n",
    "\n",
    "            # Decode the HTML\n",
    "            raw_response = response.read()\n",
    "            print(\"Webpage crawled\")\n",
    "            return raw_response\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"FAILED\")\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb2526-b16c-44ac-828b-9c9f12642a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_webpage(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d00b88-2a4f-44b7-89c0-0b85c9eb8f4e",
   "metadata": {},
   "source": [
    "It thinks we're a bot because we're accessing the website without a web browser. We can fix this though via the 'headers' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78734b8-7581-46b8-a279-852c1d8627fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://www.college.police.uk/app\"\n",
    "request = urllib.request.Request(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "\n",
    "crawl_webpage(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd005ed-4db0-4a10-a53c-647cc10c756a",
   "metadata": {},
   "source": [
    "The webpage is read as a series of bytes, which makes it very hard to read. Let's convert it to characters via utf-8, the format most websites are encoded in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8ff65-be4d-4cb6-bf07-d4124c0557c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decoding it makes it much more legible\n",
    "# Try it a couple of times if you get an error. The CoP website is inconsistent when it comes to blocking webscrapers.\n",
    "\n",
    "cleaned_webpage = crawl_webpage(request).decode('utf-8')\n",
    "print(cleaned_webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c032a28-2456-4c25-b3dd-efbca0c01f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It's filled with a lot of tags we don't need. We'll fix that with Beautiful Soup \n",
    "soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83eed4-30c2-4a51-93ec-61f39b283f97",
   "metadata": {},
   "source": [
    "### 1.2 | Save webpage as file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d68b8a-af5a-43ae-9f36-1fc0c7361b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a folder to save our output to\n",
    "FILE_PATH = \"res/data_mining/first_webpage\"\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(FILE_PATH, ignore_errors=True)\n",
    "    os.makedirs(FILE_PATH)\n",
    "except:\n",
    "    print(f\"Error. The directory {FILE_PATH} already exists and cannot be deleted. Please delete it manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953b92d-371d-4cd1-939d-669e0c641960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the website text as a file, so we can access it later\n",
    "with open(f\"{FILE_PATH}/website_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f05d4-b7e5-4b24-930f-b44dca59ed49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_and_save_website_images(soup):\n",
    "\n",
    "    # Find all image tags\n",
    "    img_tags = soup.find_all('img')\n",
    "    print(img_tags)\n",
    "        \n",
    "    # Download and save each image\n",
    "    for img_tag in img_tags:\n",
    "    \n",
    "        # Handle relative URLs by joining them with the base URL\n",
    "        img_url = img_tag['src']\n",
    "        img_url = urljoin(url, img_url)\n",
    "    \n",
    "        # Get the image data\n",
    "        img_data = requests.get(img_url).content\n",
    "    \n",
    "        # Extract the image filename from the URL\n",
    "        img_filename = os.path.join(FILE_PATH, os.path.basename(img_url))\n",
    "    \n",
    "        # Save the image to a file\n",
    "        with open(img_filename, 'wb') as img_file:\n",
    "            img_file.write(img_data)\n",
    "            print(f\"Image saved: {img_filename}\")\n",
    "\n",
    "find_and_save_website_images(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cde5c7-f9d5-4c89-81ff-22e611b234db",
   "metadata": {},
   "source": [
    "### 1.3 | Web Scraping Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9a87d-3587-477e-b10a-fee81abb4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the URL with your website\n",
    "my_url = \"https://www.netflix.com/gb/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5abb68-2ed4-4dac-8a60-11e15453878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # First, let's get the website's text\n",
    "    request = urllib.request.Request(my_url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(requests.get(my_url).text, \"html.parser\")\n",
    "\n",
    "    # Second, let's create a file to save it to\n",
    "    FILE_PATH = \"res/data_mining/my_webpage/\"\n",
    "    if not os.path.isdir(FILE_PATH):\n",
    "        os.makedirs(FILE_PATH)\n",
    "\n",
    "    # Third, let's save the website's text as a file\n",
    "    with open(f\"{FILE_PATH}/website_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write( soup.get_text() )\n",
    "\n",
    "    # Finally, let's save the website's images\n",
    "    find_and_save_website_images(soup )\n",
    "except Exception as e:\n",
    "    print(\"Invalid website: \", my_url)\n",
    "    print(\"Did you forget the 'https://' ?\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc8731-5f8a-4e27-8da1-74717af1dd70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2 | Web Scraping for Deeply Linked Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b1730-cb9b-4e23-a35e-dcb9300996b0",
   "metadata": {},
   "source": [
    "Larger websites are deeply linked: they have a lot of URLs that take you to other parts of their website. We will take this into account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b5a95-ebc0-4770-aa63-9da9cf0acfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8266b38-22f1-4af3-bc4a-860db95b0d9f",
   "metadata": {},
   "source": [
    "### 2.1 | Scrape Website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7e2db-9ffe-482e-a33f-eb6618285bfa",
   "metadata": {},
   "source": [
    "Essentially, we'll be scraping the webpage as normal. However, we're going to also scrape any internal hyperlinks that it has. These are links to other parts of the webpage, and we'll scrape those as well. Scraping internal hyperlinks requires a lot of custom code that is beyond the scope of this class, so we'll stick to a high-level overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e9f08-2822-41e4-9fc1-775acd1e68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to need to create our own HTML parser to extract all the hyperlinks\n",
    "# This will require is to inherit from 'HTMLParser' and override a function.\n",
    "# It's far beyond the scope of the class. Please just run this code, though feel free to ask questions.\n",
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcae2cf-c5d2-46ba-aeb7-b6be1c0d73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL. It's very similar to §1.1\n",
    "\n",
    "def get_hyperlinks(url):\n",
    "\n",
    "    request = urllib.request.Request(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    \n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(request) as response:\n",
    "\n",
    "            # If the response is not HTML, return an empty list\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # Decode the HTML\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce9eca-590e-4bc0-92ea-5b285161bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # Parse the URL and check if the domain is the same\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b19ee0-51c8-40cc-8e1f-23335dfbe615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url, MAX_NUMBER_OF_URLS_TO_CRAWL = None, verbose = False):\n",
    "    \n",
    "    # Parse the URL and get the domain\n",
    "    FILE_PATH = \"res/data_mining/\"\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = set([url])\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    shutil.rmtree(f\"{FILE_PATH}{local_domain}\", ignore_errors=True)\n",
    "    if not os.path.exists(f\"{FILE_PATH}{local_domain}/\"):\n",
    "            os.mkdir(f\"{FILE_PATH}{local_domain}/\")\n",
    "\n",
    "    if not os.path.exists(f\"{FILE_PATH}{local_domain}/text_raw/\"):\n",
    "            os.mkdir(f\"{FILE_PATH}{local_domain}/text_raw/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(f\"{FILE_PATH}{local_domain}/text_cleaned/\"):\n",
    "            os.mkdir(f\"{FILE_PATH}{local_domain}/text_cleaned/\")\n",
    "\n",
    "    # While the queue is not empty, continue crawling\n",
    "    counter = 0\n",
    "    while queue:\n",
    "\n",
    "        # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "\n",
    "        # Debugging\n",
    "        print(f\"URL #{counter} : {url}\") # see progress\n",
    "        counter += 1\n",
    "\n",
    "        if MAX_NUMBER_OF_URLS_TO_CRAWL:\n",
    "            if counter > MAX_NUMBER_OF_URLS_TO_CRAWL:\n",
    "                break\n",
    "\n",
    "        # Save text from the url to a <url>.txt file\n",
    "        if len(url) < 500:\n",
    "            try:\n",
    "                with open(FILE_PATH+local_domain+'/text_raw/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "\n",
    "                    # Get the text from the URL using BeautifulSoup\n",
    "                    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "                    # Get the text but remove the tags\n",
    "                    text = soup.get_text()\n",
    "\n",
    "                    # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "                    if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                        print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "                \n",
    "                    # Otherwise, write the text to the file in the text directory\n",
    "                    f.write(text)\n",
    "\n",
    "                # Get the hyperlinks from the URL and add them to the queue\n",
    "                for link in get_domain_hyperlinks(local_domain, url):\n",
    "                    if link not in seen:\n",
    "                        queue.append(link)\n",
    "                        seen.add(link)\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"--> Found new hyperlink! {link}\")\n",
    "            except:\n",
    "                print(f\"URL not valid: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8161fa4-f282-44f3-bec0-0a68b2925564",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.reddit.com/r/SuicideWatch/\"\n",
    "\n",
    "crawl(url, MAX_NUMBER_OF_URLS_TO_CRAWL = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b7a3d-5b1c-491c-b3bf-cbbda087d49d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3 | Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a149e-55bb-45ab-b679-209cb941bf4e",
   "metadata": {},
   "source": [
    "These webpages contain a lot of blank space and information we don't need, so we'll need to clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d30cbc-ef88-4357-926e-d7a136fc484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get all our files before we do anything\n",
    "DIRECTORY = \"res/data_mining/www.college.police.uk\"\n",
    "files = os.listdir(f\"{DIRECTORY}/text_raw\")\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f61a68-4d09-468e-99ee-41ccff96123a",
   "metadata": {},
   "source": [
    "### 3.1 | Problem of Blank Space, Illustrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc2636-67e6-414a-bf62-63f8068830dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's see how much blank space our files contain.\n",
    "with open(f\"{DIRECTORY}/text_raw/{files[0]}\", \"r\", encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f429f32-f922-4f35-bb6b-18f308c3c748",
   "metadata": {},
   "source": [
    "### 3.2 | Remove Blank Space (and some irrelevant information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f7955-e8ed-4b54-9dc8-565c14664bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes all the way new lines or redundant spaces could be present\n",
    "def remove_newlines(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab60ab-94b1-4382-b4ee-4ba7cfa726ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_texts=[]\n",
    "\n",
    "# Get all the text files in the text directory\n",
    "for individual_file in files:\n",
    "\n",
    "    # Ignores the checkpoints folder\n",
    "    if \".ipynb_checkpoints\" in individual_file:\n",
    "        continue\n",
    "\n",
    "    # Open the raw text file\n",
    "    with open(f\"{DIRECTORY}/text_raw/{individual_file}\", \"r\", encoding=\"UTF-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Cleans the raw text file\n",
    "        text = remove_newlines(text)\n",
    "        title = individual_file[11:-4].replace('-',' ').replace('_', ' ').replace('#update','')\n",
    "\n",
    "        # Saves it to a list, so we can turn it into a dataframe later.\n",
    "        cleaned_texts.append((title, text))\n",
    "\n",
    "\n",
    "        # Saves it as a raw file\n",
    "        with open(f\"{DIRECTORY}/text_cleaned/{individual_file}\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write( text )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e186346-5907-43c1-a707-acb218119266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The problem of blank space is now fixed! Let's see an example\n",
    "with open(f\"{DIRECTORY}/text_cleaned/{files[0]}\", \"r\", encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d77368-ad32-4935-9d96-11d7a317a4bc",
   "metadata": {},
   "source": [
    "### 3.3 | Saves our Cleaned Text as Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c989dd-f202-42f7-a56c-68b702e784e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our cleaned_text is a list of touples in the format of (title, text). \n",
    "print(\"EXAMPLE\")\n",
    "print(f\"Title = {cleaned_texts[0][0]}\\nText = {cleaned_texts[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4eb964-2c68-4cfa-82de-5d1d0086dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cleaned_texts, columns = ['title', 'text'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5e251-fa99-49a4-b312-ffbc07779df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{DIRECTORY}/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849db89f-9028-436a-a821-8e103d465084",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4 | Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19dd81b-b05c-454c-94bb-b7c3c11675bc",
   "metadata": {},
   "source": [
    "The college of policing website doesn't contain anything particularly juicy, in need of analysis. Instead, we'll turn to a far juicier place: Reddit! We'll look at posts that may contain suicidal messages. These are post that may deserve law enforcement attention. These posts have already been scraped (§2) and cleaned (§3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b128ad-e8c0-4b94-80de-8247377b8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import neattext.functions as nfx\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7029a-608f-43f0-a4f2-a0c7556099fd",
   "metadata": {},
   "source": [
    "### 4.1 | Data Reddit Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258fa71-3f9a-4ab0-a47b-a780ad2a87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('res/data_mining/reddit_suicides_abridged.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93dccf1-c544-4b36-ac75-d4aac13cbb22",
   "metadata": {},
   "source": [
    "This webscraped dataset looks pretty similar to the webscraped dataset that we scraped ourselves in §2.0, albeit the contents are different. Let's see what it looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7db051-f7b5-4f38-8e49-23ae4e09df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a non-suicidal text\n",
    "df[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcee392-f233-4898-bfde-7b9ef5d3bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a text that discusses suicdie\n",
    "df[\"text\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005bd5a-463e-4ef4-b5f3-ead21b027308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many posts we have\n",
    "df[\"text\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc04f1-f075-4e90-904c-2195f2b84f18",
   "metadata": {},
   "source": [
    "Wow. 1,000 posts. That's a bit too much to read by hand, so let's analyze it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec7a0a-a9cb-4b2e-b4b3-2793eb3eada7",
   "metadata": {},
   "source": [
    "### 4.2 | Transformation #1: Language Transformations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeee818-de01-4417-8f12-d871e35e97fc",
   "metadata": {},
   "source": [
    "There are a lot of meaningless words in language, words that tell us little about the content. We need to remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82914a6-3260-48f8-bb9f-cde06cd62b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = []\n",
    "\n",
    "    # Loops through all the text\n",
    "    for ind_text in text:\n",
    "\n",
    "        # Make all the text lowercase so our program doesn't think 'Wife', 'wIFE', and 'WIFE' are seperate words\n",
    "        ind_text=ind_text.lower()\n",
    "\n",
    "        # Removes odd characters\n",
    "        ind_text=nfx.remove_special_characters(ind_text)\n",
    "\n",
    "        # Removes insignificant words\n",
    "        ind_text=nfx.remove_stopwords(ind_text)\n",
    "        \n",
    "        cleaned_text.append(ind_text)\n",
    "        \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b5278-e62a-484b-af85-d7209feba410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_cleaned\"] =clean_text(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775b30f-fe75-41d9-ac82-77a252030b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print out an example cleaned vs dirty text \n",
    "\n",
    "sample_dirty = df[\"text\"][0]\n",
    "sample_clean = df[\"text_cleaned\"][0]\n",
    "\n",
    "print(f\"{'*'*30}\\nSAMPLE DIRTY TEXT\\n{'*'*30}\\n{ sample_dirty }\\n\\n\")\n",
    "print(f\"{'*'*30}\\nSAMPLE CLEAN TEXT\\n{'*'*30}\\n{ sample_clean }\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a82c9-e169-43cf-87ce-bd71db8a2201",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3 | Transformatin #2: Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f51a79-ee86-49fe-9eba-d3c2356415eb",
   "metadata": {},
   "source": [
    "Machines have a hard time reading words. We'll convert them to numbers to make them more conducive to later analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ffad1-923a-45bf-8600-b257b8379e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each word a unique ID and count it via CountVectorizor\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_text_df = vectorizer.fit_transform(df[\"text_cleaned\"])\n",
    "\n",
    "# Temp is inaccessible, so we'll need to do some dataset conversions to use it\n",
    "vectorized_text_df = pd.DataFrame.sparse.from_spmatrix(vectorized_text_df) # sparse matrix\n",
    "vectorized_text_df = vectorized_text_df.sparse.to_dense() # dense matrix  (for PCA)\n",
    "\n",
    "vectorized_text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320eb5e-bd97-48f6-bda8-36a12f666c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the size\n",
    "vectorized_text_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef2ecf-e2ed-471c-a2ce-f6b5f8df2812",
   "metadata": {},
   "source": [
    "Looks like there were around 10,000 unique words, represented as unique IDs. Thus, each row now has about 10,000 columns, with each column counting the frequency of a particular word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd16fe-dc25-4225-9869-b78774f806d4",
   "metadata": {},
   "source": [
    "It may look a little scary with all these zeros, but this is completely normal! There are ~10,000 unique words, and not every post will have 10,000 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342433d0-2d48-4e43-90e0-ec9741453f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at columns with a non-zero value\n",
    "\n",
    "# Get all columns with a non zero value for the first row (i.e., the first post)\n",
    "indices_of_interest = vectorized_text_df.columns.get_indexer( vectorized_text_df.columns[vectorized_text_df.iloc[0] != 0] )\n",
    "\n",
    "# Print them\n",
    "vectorized_text_df.iloc[0:2, indices_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e89c79-c41f-4811-9bd1-fc42054607a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5 | Run & Interpret PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019eae1-3902-47c1-b55f-0d51417157a6",
   "metadata": {},
   "source": [
    "### 5.1 | Runs the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8a9af-3e25-4b82-83ad-8077cdf1dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_results = pca.fit_transform(vectorized_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ce471-8644-4dfa-8535-65eb28fb75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are saved as a numpy array, so let's convert it to a pandas dataframe for legibility\n",
    "pca_df = pd.DataFrame(data = {\"PCA1\" : pca_results[: , 0],\n",
    "                             \"PCA2\" : pca_results[: , 1],\n",
    "                             \"raw_text\" : df[\"text\"] })\n",
    "pca_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac306a7-2f78-4297-834e-79bf23030881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Principal Component - 1',fontsize=20)\n",
    "plt.ylabel('Principal Component - 2',fontsize=20)\n",
    "plt.title(\"Principal Component Analysis of Text Data\",fontsize=20)\n",
    "\n",
    "#Suicides\n",
    "plt.scatter(pca_df[\"PCA1\"], pca_df[\"PCA2\"], c='black', s=1)\n",
    "\n",
    "\n",
    "plt.xlim(-.5, 0)\n",
    "plt.ylim(-.5, 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134ce69-4202-4518-8bf9-739c748afb8a",
   "metadata": {},
   "source": [
    "### 5.2 | Interpret PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546bdd4b-7bcb-4fcc-b1a2-80dbdf1e7b73",
   "metadata": {},
   "source": [
    "This is what our unstructured text look like. They appear to exist on a spectrum, from the lower left to upper right. It's unclear what this spectrum means. We'd need to sample text on both sides of the spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811fd36-1b19-4532-82d0-7d6e77a10b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's look at a post from the lower left\n",
    "list_of_extreme_datapoints = pca_df[(pca_df[\"PCA1\"] < -0.4) & (pca_df[\"PCA2\"] < -0.4)].index\n",
    "pca_df[\"raw_text\"][list_of_extreme_datapoints[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2672e6-7f4e-462d-9fd8-7fcac7025f17",
   "metadata": {},
   "source": [
    "This looks extremely dark. Maybe the lower left are those texts that are the most suicidal. Let's try the other extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5650450-b22e-4172-9d5c-f8926cad746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a post from the upper right\n",
    "list_of_extreme_datapoints = pca_df[(pca_df[\"PCA1\"] > -0.2) & (pca_df[\"PCA2\"] > -0.2)].index\n",
    "pca_df[\"raw_text\"][list_of_extreme_datapoints[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1bd958-4bd2-47e4-8594-35e20fc4d983",
   "metadata": {},
   "source": [
    "This text looks pretty innocent! It looks like the PCA arranged this text based on HOW SUICIDAL IT IS: the more suicidal a post is, the closer to the lower left it will be, and the smaller the PCA1 and PCA2 values will be "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26d560-1a48-42bd-8370-e91f2dc315a9",
   "metadata": {},
   "source": [
    "### 5.3 | Double Check our PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2def582-5038-47ca-a2f2-0f3c45906c63",
   "metadata": {},
   "source": [
    "There is something we failed to disclosed earlier: this text data is ALREADY LABELLED! In other words, someone already went through these post and labelled them, one by one, to determine if they are suicidal. We hid the label in a different file, in the addendum file, so that you can learn how to analyze unstructured text data without any aid whatsoever. However, now that you already analyzed this data, let's use it to double check our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b85f1-d619-4c19-a21b-5c2ae85e9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_key = pd.read_csv('res/data_mining/reddit_suicides_abridged_addendum.csv', encoding='utf-8')\n",
    "pca_df[\"answer_key\"] = answer_key\n",
    "\n",
    "pca_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dc2b1-6a7e-4568-ab4f-97347aba26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Principal Component - 1',fontsize=20)\n",
    "plt.ylabel('Principal Component - 2',fontsize=20)\n",
    "plt.title(\"Principal Component Analysis of Text Data\",fontsize=20)\n",
    "\n",
    "#Suicides\n",
    "plt.scatter(pca_df[pca_df[\"answer_key\"] == \"suicide\"][\"PCA1\"],\n",
    "            pca_df[pca_df[\"answer_key\"] == \"suicide\"][\"PCA2\"],\n",
    "            c='r', s=1)\n",
    "\n",
    "#Non Suicides\n",
    "plt.scatter(pca_df[pca_df[\"answer_key\"] == \"non-suicide\"][\"PCA1\"],\n",
    "            pca_df[pca_df[\"answer_key\"] == \"non-suicide\"][\"PCA2\"],\n",
    "            c='g', s=1)\n",
    "\n",
    "plt.xlim(-.5, 0)\n",
    "plt.ylim(-.5, 0)  \n",
    "\n",
    "plt.legend([\"Suicides\", \"Non-Suicides\"],prop={'size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765a8da-14f4-4f32-b7fa-4d7059c6128c",
   "metadata": {},
   "source": [
    "Well done. It looks like our interpretation is faithful to the answeer key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08d440-4cde-4d25-8c9d-2efef792db86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6 | Miscellaneous: Visualize Words (for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034246eb-c74d-461c-894e-52d2e37d2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Gets non suicides text\n",
    "suicides = pca_df[pca_df['answer_key'] == 'suicide']\n",
    "suicides_text = ' '.join(suicides['raw_text'].astype(str))\n",
    "\n",
    "# Displays non suicides\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(suicides_text)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2357af-1811-4c74-916c-b16498c402fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets non suicides text\n",
    "non_suicides = pca_df[pca_df['answer_key'] == 'non-suicide']\n",
    "non_suicides_text = ' '.join(non_suicides['raw_text'].astype(str))\n",
    "\n",
    "# Displays non suicides\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(non_suicides_text)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41803dde-cd83-4b01-8e51-b1f959baa2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
